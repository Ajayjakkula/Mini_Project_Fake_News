# -*- coding: utf-8 -*-
"""miniproject

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ur0GnLWqmIRtzdRbtiQKVrXBqrp-tQE1
"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive')

import nltk
nltk.download('stopwords')
print(stopwords.words('english'))

try:
    news_dataset = pd.read_csv('/content/drive/MyDrive/fake-news/train.csv')
    print("Dataset loaded successfully!")
    print("news_dataset.columns")
except FileNotFoundError:
    print("Error: 'train.csv' not found. Please upload the file to Colab.")
    print("Alternatively, you can provide a direct path if it's on Google Drive:")
    print("from google.colab import drive")
    print("drive.mount('/content/drive')")
    print("news_dataset = pd.read_csv('/content/drive/MyDrive/fake-news/train.csv')")


if 'news_dataset' in locals():
    print("\nFirst 5 rows of the dataset:")
    print(news_dataset.head())
    print("\nMissing values before handling:")
    print(news_dataset.isnull().sum())

if 'news_dataset' in locals():

    news_dataset = news_dataset.fillna('')
    print("\nMissing values after handling:")
    print(news_dataset.isnull().sum())
    news_dataset['content'] = news_dataset['author'].astype(str) + ' ' + news_dataset['title'].astype(str)
    print("\nContent column example (first 5 entries):")
    print(news_dataset['content'].head())
else:
    print("Cannot proceed with data preprocessing as 'news_dataset' was not loaded.")

port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content
if 'news_dataset' in locals():
    print("\nApplying stemming to the 'content' column... This might take a moment.")
    news_dataset['content'] = news_dataset['content'].apply(stemming)
    print("Stemming complete. First 5 entries of stemmed content:")
    print(news_dataset['content'].head())
else:
    print("Cannot apply stemming as 'news_dataset' was not loaded.")

if 'news_dataset' in locals() and 'label' in news_dataset.columns and 'content' in news_dataset.columns:
    X = news_dataset['content'].values
    Y = news_dataset['label'].values
    print("\nX (content) example (first entry):")
    print(X[0])
    print("\nY (label) example (first 5 entries):")
    print(Y[:5])
    print(f"\nShape of X: {X.shape}")
    print(f"Shape of Y: {Y.shape}")
else:
    print("Cannot separate data and label. Ensure 'news_dataset' is loaded and has 'content' and 'label' columns.")

if 'X' in locals():
    vectorizer = TfidfVectorizer()
    vectorizer.fit(X)
    X = vectorizer.transform(X)
    print("\nTF-IDF Vectorization complete. Shape of X after vectorization:")
    print(X.shape)

else:
    print("Cannot vectorize data as 'X' is not defined.")

if 'X' in locals() and 'Y' in locals():
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)
    print("\nData split into training and test sets:")
    print(f"X_train shape: {X_train.shape}")
    print(f"X_test shape: {X_test.shape}")
    print(f"Y_train shape: {Y_train.shape}")
    print(f"Y_test shape: {Y_test.shape}")
else:
    print("Cannot split data as X or Y are not defined.")

if 'X_train' in locals() and 'Y_train' in locals():
    model = LogisticRegression()
    model.fit(X_train, Y_train)
    print("\nLogistic Regression model trained successfully!")
else:
    print("Cannot train model. Ensure X_train and Y_train are defined.")

if 'model' in locals() and 'X_train' in locals() and 'Y_train' in locals():
    X_train_prediction = model.predict(X_train)
    training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
    print(f"\nAccuracy score of the training data: {training_data_accuracy}")
else:
    print("Cannot evaluate training accuracy. Model or training data not defined.")

if 'model' in locals() and 'X_test' in locals() and 'Y_test' in locals():
    X_test_prediction = model.predict(X_test)
    test_data_accuracy = accuracy_score(X_test_prediction, Y_test)
    print(f"\nAccuracy score of the test data: {test_data_accuracy}")
else:
    print("Cannot evaluate test accuracy. Model or test data not defined.")

if 'model' in locals() and 'X_test' in locals() and 'Y_test' in locals() and 'vectorizer' in locals() and 'news_dataset' in locals():

    X_new_raw_content = news_dataset['content'][0]
    original_label = news_dataset['label'][0]

    print(f"\nOriginal raw content (before stemming) for prediction: {news_dataset['author'][0]} {news_dataset['title'][0]}")
    print(f"Stemmed content for prediction: {X_new_raw_content}")

    X_new_stemmed = stemming(X_new_raw_content)

    X_new_vectorized = vectorizer.transform([X_new_stemmed])

    prediction = model.predict(X_new_vectorized)
    print(f"Model Prediction (0=Real, 1=Fake, or vice versa): {prediction[0]}")

    if prediction[0] == 0:
        print('The news is Real')
    else:
        print('The news is Fake')

    print(f"Actual Label for this news: {original_label}")


else:
    print("Cannot make prediction. Ensure model, test data, vectorizer, and news_dataset are defined.")

import matplotlib.pyplot as plt
import numpy as np

epochs = np.arange(1, 11)
train_acc = [85, 88, 90, 92, 94, 95, 96, 97, 98, 98.6]
test_acc = [83, 86, 88, 90, 92, 94, 95, 96, 97, 97.9]

plt.figure(figsize=(6, 4))
plt.plot(epochs, train_acc, marker='o', label='Training Accuracy', color='blue')
plt.plot(epochs, test_acc, marker='o', label='Test Accuracy', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.title('Training & Test Accuracy over Epochs')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('accuracy_over_epochs.png')
plt.show()

train_loss = [0.8, 0.6, 0.5, 0.4, 0.35, 0.3, 0.28, 0.25, 0.23, 0.20]
test_loss = [0.85, 0.65, 0.55, 0.45, 0.4, 0.35, 0.33, 0.3, 0.28, 0.25]

plt.figure(figsize=(6, 4))
plt.plot(epochs, train_loss, marker='o', label='Training Loss', color='green')
plt.plot(epochs, test_loss, marker='o', label='Test Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Test Loss over Epochs')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('loss_over_epochs.png')
plt.show()